{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-preparation for Prediction of \"Sachkonto\" - from Table SAP BSAK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils_bsak import printSamplesFromSaktos\n",
    "from utils_bsak import is_date_column, is_decimal_column, convert_column_decimal2float\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils_bsak import target_min_value_records\n",
    "\n",
    "import joblib\n",
    "\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data & First Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Names and Paths for Loading Raw-Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files for Data-Loading\n",
    "\n",
    "folder_raw_data = \"../data_raw\"\n",
    "\n",
    "# I have three files containing data:\n",
    "file_csv1 = r\"\\Export_bsak_0124-prctr.csv\".replace(\"\\\\\", \"/\")\n",
    "file_csv2 = r\"\\Export_bsak_0224-prctr.csv\".replace(\"\\\\\", \"/\")\n",
    "file_csv3 = r\"\\export_bsak_010123-311223-SaktoExclude.csv\".replace(\"\\\\\", \"/\")\n",
    "\n",
    "files_csv = [file_csv1, file_csv2, file_csv3]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Names and Paths for Writting Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files for Preprocessed Data\n",
    "\n",
    "model_name = \"Sachkonto_stratified\" # name of the model that is going to be used\n",
    "\n",
    "folder_preprocessed_data = \"../data_preprocessed/\"\n",
    "file_joblib_data = 'Data_' + model_name + '.pkl'\n",
    "path_joblib_data = folder_preprocessed_data + file_joblib_data # path for dumping preprocessed data\n",
    "folder_web_mappings = '../demo_web_page/mappings' # folder for encoder-mapping etc. to be used by web-page\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for _file in files_csv:\n",
    "    path_data = folder_raw_data + _file\n",
    "    df_new = pd.read_csv(path_data, sep=';', encoding='latin1')\n",
    "    df = pd.concat([df, df_new], ignore_index=True)\n",
    "\n",
    "# Erste Zeilen anzeigen, um sicherzustellen, dass die Daten korrekt geladen wurden\n",
    "print(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentage Distribution of Target-Values: \"Sachkonto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage of data for a specific 'Sachkonto'\")\n",
    "print(df[\"Sachkonto\"].value_counts(normalize=True) * 100)\n",
    "print(\"--------------------------------\")\n",
    "print(\"Count of data for a specific 'Sachkonto'\")\n",
    "print(df[\"Sachkonto\"].value_counts())\n",
    "print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Sachkonto'\n",
    "proceed = target in df.columns\n",
    "#proceed = False\n",
    "\n",
    "if proceed:\n",
    "    # drop empty, i.e. nan-only rows:\n",
    "    df = df.dropna(axis=0, how='all')\n",
    "\n",
    "\n",
    "    # drop empty, i.e. nan-only columns:\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "\n",
    "\n",
    "    # drop duplicate rows:\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "\n",
    "    # drop rows with target being nan:\n",
    "    df = df.dropna(subset=[target], axis=0)\n",
    "    \n",
    "\n",
    "    # keep only rows with target not being empty string:\n",
    "    df = df[df[target] != \"\"]\n",
    "    \n",
    "\n",
    "    # keep only columns with non-constant value:\n",
    "    df = df.loc[:, df.nunique() > 1]\n",
    "\n",
    "\n",
    "    # remove columns containing dates - no time-series analysis components in this notebook:\n",
    "    # define a function to identify date columns:\n",
    "    import re\n",
    "\n",
    "    date_cols = [col for col in df.columns if is_date_column(df[col])]\n",
    "    df = df.drop(columns=date_cols)\n",
    "\n",
    "    # Identify decimal columns:\n",
    "    decimal_cols = [col for col in df.columns if is_decimal_column(df[col])]\n",
    "\n",
    "    # Convert decimal-colums to floats:\n",
    "    for col in decimal_cols:\n",
    "        df[col] = convert_column_decimal2float(df[col])\n",
    "\n",
    "    # Label-encode all columns of type \"object\":\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    columns_to_encode = df.dtypes[df.dtypes == \"object\"].index.to_list()\n",
    "    column_encoders = {}\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    for col in columns_to_encode:\n",
    "        label_encoder = LabelEncoder()\n",
    "        df[col] = label_encoder.fit_transform(df[col].astype(str))\n",
    "        # save encoding for each column\n",
    "        inverse_dict = { k: i for i,k in enumerate(label_encoder.classes_)}\n",
    "        column_encoders[col] = inverse_dict\n",
    "\n",
    "\n",
    "    # drop columns that contain only identifiers (that we do not want to analyze further here):\n",
    "    identifier_columns = [\"Referenz\", \"Ausgleichsbeleg\", \"Zuordnung\", \"Belegnummer\", \"Auftrag\", \"Einkaufsbeleg\", \"Rechnungsbezug\"]\n",
    "    \n",
    "    for id_col in identifier_columns:\n",
    "        if id_col in df.columns:\n",
    "            df = df.drop(id_col, axis='columns')\n",
    "   \n",
    "    # finally drop columns that contain data that we can not / do not want to analyze - drop this columns:\n",
    "    # 'Text': to evaluate this would require a model on it's own - not (yet) justified\n",
    "    # 'Zuordnung': possible target-leakage?\n",
    "    # 'Hauptbuchkonto' : possible target-leakage? won't be available, when process is in frontend anyway.\n",
    "\n",
    "    unwanted_columns = [\"Text\", \"Zuordnung\", \"Hauptbuchkonto\"]\n",
    "    nan_columns = list(df.columns[df.isna().sum() >0].values)\n",
    "    print(f\"nan_columns: {nan_columns}\")\n",
    "    unwanted_columns.extend(nan_columns)\n",
    "    for u_col in unwanted_columns:\n",
    "        if u_col in df.columns:\n",
    "            df.drop(columns=[u_col], inplace=True)\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split & Feature Selection with Boruta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Train-Test Split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratified split of data\n",
    "\n",
    "# for stratification, all target classes have to have more than 1 record - we choose 4 as minimum here:\n",
    "stratifiable_target_values = target_min_value_records(dataframe=df, target_column=target, min_value_records=4).astype(int)\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "print(f\"df.shape with all targets: {df.shape}\")\n",
    "df = df[df[target].isin(stratifiable_target_values)]\n",
    "print(f\"df.shape with stratifiable targets: {df.shape}\")\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "# prepare target encoding:\n",
    "target_label_encoder = LabelEncoder()\n",
    "y = df[target]\n",
    "target_label_encoder.fit(y)\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "print(f\"df_train.shape : {df_train.shape}\")\n",
    "print(f\"df_test.shape : {df_test.shape}\")\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "# split target from data:\n",
    "X_train = df_train.drop(target, axis=1)\n",
    "X_test = df_test.drop(target, axis=1)\n",
    "y_train = target_label_encoder.transform(df_train[target].values)\n",
    "y_test = target_label_encoder.transform(df_test[target].values)\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "print(\"y_train unique values:\")\n",
    "print(np.sort(np.unique(y_train)))\n",
    "print(\"y_test unique values:\")\n",
    "print(np.sort(np.unique(y_test)))\n",
    "print(\"--------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info needed for webpage script.js:\n",
    "\n",
    "Steuerkennzeichen_dict = column_encoders['Steuerkennzeichen']\n",
    "Steuerkennzeichen_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info needed for webpage script.js:\n",
    "\n",
    "target_dict = { i: k for i,k in enumerate(target_label_encoder.classes_)}\n",
    "target_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store the Transformed and Encoded Data and Labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data and LabelEncoder for later use:\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data and LabelEncoder for later use:\n",
    "\n",
    "data = {\n",
    "    'X_train': X_train,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test,\n",
    "    'column_encoders' : column_encoders,\n",
    "    'target_dict': target_dict,\n",
    "    'Steuerkennzeichen_dict': Steuerkennzeichen_dict\n",
    "}\n",
    "\n",
    "joblib.dump(data, path_joblib_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export some Data to JSON for the Web-Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "import os\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\"\n",
    "    Transforms NumPy-types to JSON compatible Python types.\n",
    "    \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (np.integer, np.int64)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.floating, np.float32, np.float64)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, (np.ndarray,)):\n",
    "            return obj.tolist()\n",
    "        return super().default(obj)\n",
    "\n",
    "def export_mapping_for_web(joblib_file_path, output_folder, keys_to_export):\n",
    "    \"\"\"\n",
    "    Loads a joblib file, extracts certain keys and exports them to single JSON-files.\n",
    "    \n",
    "    :param joblib_file_path: path to a .joblib-file containing the data\n",
    "    :param output_folder: folder to write the JSON-files to\n",
    "    :param keys_to_export: list of keys (e.g. ['target_dict', 'Steuerkennzeichen_dict'])\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    data = joblib.load(joblib_file_path)\n",
    "\n",
    "    for key in keys_to_export:\n",
    "        if key in data:\n",
    "            output_path = os.path.join(output_folder, f\"{key}.json\")\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(data[key], f, ensure_ascii=False, indent=2, cls=NumpyEncoder)\n",
    "                print(f\"{key} exported to {output_path}\")\n",
    "        else:\n",
    "            print(f\"Key '{key}' not found in data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the export:\n",
    "\n",
    "export_keys = [\"target_dict\", \"Steuerkennzeichen_dict\"] # what to export into single json files named like this keys\n",
    "\n",
    "export_mapping_for_web(path_joblib_data, folder_web_mappings, export_keys)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CondaSapBSAKEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
